# =============================================================================
# Monkey-Bot Framework Configuration
# =============================================================================
# Copy this file to .env and fill in your credentials

# =============================================================================
# Environment Configuration
# =============================================================================
# Environment mode: "development" | "production"
# - development: Loads secrets from .env file
# - production: Loads secrets from GCP Secret Manager
ENVIRONMENT=development

# =============================================================================
# Gateway Configuration
# =============================================================================
# Comma-separated list of allowed user emails (Google Chat senders)
# Only these emails can interact with the bot
ALLOWED_USERS=user1@example.com,user2@example.com

# =============================================================================
# Google Cloud Configuration
# =============================================================================
# Path to GCP service account JSON key file (required for Vertex AI and GCS)
# Create service account: https://console.cloud.google.com/iam-admin/serviceaccounts
# Required IAM roles: aiplatform.user, secretmanager.secretAccessor, run.invoker
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# GCP Project ID for Vertex AI (required for Google Vertex AI models)
# Find yours at: https://console.cloud.google.com/home/dashboard
VERTEX_AI_PROJECT_ID=your-gcp-project-id

# GCP Project ID for production resources (only if different from VERTEX_AI_PROJECT_ID)
# Used by Secret Manager and other GCP services in production
# GCP_PROJECT_ID=your-gcp-project-id

# Vertex AI region (default: us-central1)
# Must match your GCP project region
# Options: us-central1, us-east1, us-west1, europe-west1, europe-west4, asia-northeast1
# See: https://cloud.google.com/vertex-ai/docs/general/locations
VERTEX_AI_LOCATION=us-central1

# =============================================================================
# Model Configuration
# =============================================================================
# LLM provider: "google_vertexai" | "openai" | "anthropic"
# Default: google_vertexai (requires VERTEX_AI_PROJECT_ID)
# For openai: Requires OPENAI_API_KEY environment variable
# For anthropic: Requires ANTHROPIC_API_KEY environment variable
MODEL_PROVIDER=google_vertexai

# Model name/identifier (provider-specific)
# Google Vertex AI models:
#   - gemini-2.5-flash (fast, cost-effective, recommended)
#   - gemini-2.5-pro (most capable, higher cost)
#   - gemini-1.5-flash (legacy)
# OpenAI models:
#   - gpt-4 (most capable)
#   - gpt-4-turbo (fast, cost-effective)
#   - gpt-3.5-turbo (fastest, cheapest)
# Anthropic models:
#   - claude-3-opus (most capable)
#   - claude-3-sonnet (balanced)
#   - claude-3-haiku (fastest)
MODEL_NAME=gemini-2.5-flash

# Generation temperature (0.0 = deterministic, 1.0 = creative)
# - 0.0-0.3: Factual, consistent responses (good for QA, code)
# - 0.4-0.7: Balanced (good for general chat)
# - 0.8-1.0: Creative, varied responses (good for brainstorming)
MODEL_TEMPERATURE=0.7

# Maximum output tokens (affects cost and response length)
# Google Vertex AI: 8192 max
# OpenAI GPT-4: 4096 max (gpt-4-turbo: 128000 max)
# Anthropic Claude: 4096 max
MODEL_MAX_TOKENS=8192

# Optional: Custom system prompt file path (overrides default prompt)
# If not set, uses default agent prompt
# SYSTEM_PROMPT_FILE=/path/to/custom-prompt.txt

# =============================================================================
# Memory Configuration
# =============================================================================
# Local memory directory (relative to project root)
MEMORY_DIR=./data/memory

# Enable GCS sync for memory persistence
# - false: Local files only (for development)
# - true: Sync to GCS bucket (for Cloud Run)
GCS_ENABLED=false

# GCS bucket name for memory storage (required if GCS_ENABLED=true)
# Create bucket: gsutil mb gs://your-bucket-name
GCS_MEMORY_BUCKET=emonk-memory

# =============================================================================
# Agent Configuration
# =============================================================================
# Agent name (for logging/identification)
AGENT_NAME=emonk-general-assistant

# Skills directory (relative to project root)
SKILLS_DIR=./skills

# Conversation context limit (number of messages sent to LLM)
CONVERSATION_CONTEXT_LIMIT=10

# =============================================================================
# Server Configuration
# =============================================================================
# Server port (Cloud Run uses PORT=8080 automatically)
PORT=8080

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =============================================================================
# Google Chat Configuration
# =============================================================================
# Google Chat response format: "workspace_addon" | "legacy"
# - workspace_addon: Modern format for Workspace Add-ons (recommended)
# - legacy: Old format for backward compatibility
GOOGLE_CHAT_FORMAT=workspace_addon

# Google Chat webhook URL (optional, for posting messages from scheduled jobs)
# Create webhook: https://developers.google.com/chat/how-tos/webhooks
# GOOGLE_CHAT_WEBHOOK=https://chat.googleapis.com/v1/spaces/xxx/messages?key=xxx

# =============================================================================
# Scheduler Configuration
# =============================================================================
# Storage backend for scheduled jobs
# - json: Local JSON files (development/testing only)
# - firestore: Firestore with distributed locking (production recommended)
SCHEDULER_STORAGE=json

# Cron secret for authenticating /cron/tick endpoint (optional but recommended)
# If not set, only Cloud Scheduler X-Cloudscheduler header is accepted
# Generate secure secret: openssl rand -hex 32
CRON_SECRET=

# Cloud Scheduler cadence (cron format)
# Determines how often the scheduler checks for due jobs
# Common patterns:
#   * * * * *       = Every minute (default, good for testing)
#   */5 * * * *     = Every 5 minutes (production recommended)
#   0 * * * *       = Every hour
#   0 9 * * 1-5     = Weekdays at 9 AM
#   0 0 * * 0       = Every Sunday at midnight
# See: https://crontab.guru/ for cron pattern reference
SCHEDULER_CADENCE=* * * * *

# Cloud Scheduler timezone (used for interpreting cron expressions)
# Default: America/New_York
# Options: Any IANA timezone (America/Los_Angeles, Europe/London, Asia/Tokyo, etc.)
# See: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones
SCHEDULER_TIMEZONE=America/New_York

# =============================================================================
# Cloud Run Deployment (for production)
# =============================================================================
# Cloud Run region
CLOUD_RUN_REGION=us-central1

# Min instances (0 = scale to zero, 1 = always-on)
# - 0: Saves cost but has cold start latency (~2-3 seconds)
# - 1: Always warm but costs ~$7/month idle
CLOUD_RUN_MIN_INSTANCES=0

# Max instances (max concurrent containers)
CLOUD_RUN_MAX_INSTANCES=10

# Service name
CLOUD_RUN_SERVICE_NAME=emonk-agent
